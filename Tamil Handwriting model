{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bfcda71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T15:35:54.462871Z",
     "iopub.status.busy": "2025-07-20T15:35:54.461951Z",
     "iopub.status.idle": "2025-07-20T15:37:50.442610Z",
     "shell.execute_reply": "2025-07-20T15:37:50.441512Z"
    },
    "papermill": {
     "duration": 115.987755,
     "end_time": "2025-07-20T15:37:50.444749",
     "exception": false,
     "start_time": "2025-07-20T15:35:54.456994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.22.4\r\n",
      "  Downloading numpy-1.22.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: numpy\r\n",
      "  Attempting uninstall: numpy\r\n",
      "    Found existing installation: numpy 1.23.5\r\n",
      "    Uninstalling numpy-1.23.5:\r\n",
      "      Successfully uninstalled numpy-1.23.5\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cudf 23.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "cuml 23.6.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "dask-cudf 23.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\r\n",
      "chex 0.1.81 requires numpy>=1.25.0, but you have numpy 1.22.4 which is incompatible.\r\n",
      "cudf 23.6.1 requires protobuf<4.22,>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "cuml 23.6.0 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\r\n",
      "dask-cuda 23.6.0 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\r\n",
      "dask-cudf 23.6.1 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\r\n",
      "momepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "pymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.22.4 which is incompatible.\r\n",
      "pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.1 which is incompatible.\r\n",
      "raft-dask 23.6.2 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\r\n",
      "ydata-profiling 4.3.1 requires scipy<1.11,>=1.4.1, but you have scipy 1.11.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed numpy-1.22.4\r\n",
      "Found existing installation: tensorflow-io 0.31.0\r\n",
      "Uninstalling tensorflow-io-0.31.0:\r\n",
      "  Successfully uninstalled tensorflow-io-0.31.0\r\n",
      "Collecting tensorflow-io\r\n",
      "  Downloading tensorflow_io-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.6 MB)\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.6/49.6 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem==0.37.1 (from tensorflow-io)\r\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: tensorflow-io-gcs-filesystem, tensorflow-io\r\n",
      "  Attempting uninstall: tensorflow-io-gcs-filesystem\r\n",
      "    Found existing installation: tensorflow-io-gcs-filesystem 0.31.0\r\n",
      "    Uninstalling tensorflow-io-gcs-filesystem-0.31.0:\r\n",
      "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.31.0\r\n",
      "Successfully installed tensorflow-io-0.37.1 tensorflow-io-gcs-filesystem-0.37.1\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.22.4)\r\n",
      "Collecting numpy\r\n",
      "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (1.11.1)\r\n",
      "Collecting scipy\r\n",
      "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.12.0)\r\n",
      "Collecting tensorflow\r\n",
      "  Downloading tensorflow-2.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.8 MB)\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m644.8/644.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: tensorflow-io in /opt/conda/lib/python3.10/site-packages (0.37.1)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\r\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\r\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\r\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.4.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (16.0.0)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.20.3)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.31.0)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (59.8.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.3.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.6.3)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.14.1)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.51.1)\r\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\r\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting keras>=3.5.0 (from tensorflow)\r\n",
      "  Downloading keras-3.10.0-py3-none-any.whl (1.4 MB)\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting numpy\r\n",
      "  Downloading numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting h5py>=3.11.0 (from tensorflow)\r\n",
      "  Downloading h5py-3.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\r\n",
      "  Downloading ml_dtypes-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.37.1)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\r\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (13.4.2)\r\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\r\n",
      "  Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\r\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\r\n",
      "  Downloading optree-0.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (405 kB)\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m405.8/405.8 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2023.5.7)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.3)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.1)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (2.3.6)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.0.9)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\r\n",
      "Installing collected packages: namex, flatbuffers, optree, numpy, tensorboard, scipy, ml-dtypes, h5py, keras, tensorflow\r\n",
      "  Attempting uninstall: flatbuffers\r\n",
      "    Found existing installation: flatbuffers 23.5.26\r\n",
      "    Uninstalling flatbuffers-23.5.26:\r\n",
      "      Successfully uninstalled flatbuffers-23.5.26\r\n",
      "  Attempting uninstall: numpy\r\n",
      "    Found existing installation: numpy 1.22.4\r\n",
      "    Uninstalling numpy-1.22.4:\r\n",
      "      Successfully uninstalled numpy-1.22.4\r\n",
      "  Attempting uninstall: tensorboard\r\n",
      "    Found existing installation: tensorboard 2.12.3\r\n",
      "    Uninstalling tensorboard-2.12.3:\r\n",
      "      Successfully uninstalled tensorboard-2.12.3\r\n",
      "  Attempting uninstall: scipy\r\n",
      "    Found existing installation: scipy 1.11.1\r\n",
      "    Uninstalling scipy-1.11.1:\r\n",
      "      Successfully uninstalled scipy-1.11.1\r\n",
      "  Attempting uninstall: ml-dtypes\r\n",
      "    Found existing installation: ml-dtypes 0.2.0\r\n",
      "    Uninstalling ml-dtypes-0.2.0:\r\n",
      "      Successfully uninstalled ml-dtypes-0.2.0\r\n",
      "  Attempting uninstall: h5py\r\n",
      "    Found existing installation: h5py 3.9.0\r\n",
      "    Uninstalling h5py-3.9.0:\r\n",
      "      Successfully uninstalled h5py-3.9.0\r\n",
      "  Attempting uninstall: keras\r\n",
      "    Found existing installation: keras 2.12.0\r\n",
      "    Uninstalling keras-2.12.0:\r\n",
      "      Successfully uninstalled keras-2.12.0\r\n",
      "  Attempting uninstall: tensorflow\r\n",
      "    Found existing installation: tensorflow 2.12.0\r\n",
      "    Uninstalling tensorflow-2.12.0:\r\n",
      "      Successfully uninstalled tensorflow-2.12.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cudf 23.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "cuml 23.6.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "dask-cudf 23.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 2.1.3 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\r\n",
      "bqplot 0.12.39 requires numpy<2.0.0,>=1.10.4, but you have numpy 2.1.3 which is incompatible.\r\n",
      "cudf 23.6.1 requires protobuf<4.22,>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "cuml 23.6.0 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\r\n",
      "cupy 12.1.0 requires numpy<1.27,>=1.20, but you have numpy 2.1.3 which is incompatible.\r\n",
      "dask-cuda 23.6.0 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\r\n",
      "dask-cudf 23.6.1 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\r\n",
      "ibis-framework 6.0.0 requires numpy<2,>=1, but you have numpy 2.1.3 which is incompatible.\r\n",
      "momepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "numba 0.57.1 requires numpy<1.25,>=1.21, but you have numpy 2.1.3 which is incompatible.\r\n",
      "pymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 2.1.3 which is incompatible.\r\n",
      "pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.15.3 which is incompatible.\r\n",
      "raft-dask 23.6.2 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\r\n",
      "tensorflow-decision-forests 1.4.0 requires tensorflow~=2.12.0, but you have tensorflow 2.19.0 which is incompatible.\r\n",
      "tensorflow-text 2.12.1 requires tensorflow<2.13,>=2.12.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.19.0 which is incompatible.\r\n",
      "tensorflow-transform 0.14.0 requires numpy<2,>=1.16, but you have numpy 2.1.3 which is incompatible.\r\n",
      "ydata-profiling 4.3.1 requires numpy<1.24,>=1.16.0, but you have numpy 2.1.3 which is incompatible.\r\n",
      "ydata-profiling 4.3.1 requires scipy<1.11,>=1.4.1, but you have scipy 1.15.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed flatbuffers-25.2.10 h5py-3.14.0 keras-3.10.0 ml-dtypes-0.5.1 namex-0.1.0 numpy-1.25.0 optree-0.16.0 scipy-1.15.3 tensorboard-2.19.0 tensorflow-2.19.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.22.4\n",
    "!pip uninstall tensorflow-io -y\n",
    "!pip install tensorflow-io\n",
    "!pip install --upgrade numpy scipy tensorflow tensorflow-io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04712ede",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T15:37:50.490910Z",
     "iopub.status.busy": "2025-07-20T15:37:50.490570Z",
     "iopub.status.idle": "2025-07-20T15:38:21.374806Z",
     "shell.execute_reply": "2025-07-20T15:38:21.373741Z"
    },
    "papermill": {
     "duration": 30.909342,
     "end_time": "2025-07-20T15:38:21.376824",
     "exception": false,
     "start_time": "2025-07-20T15:37:50.467482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.25.0)\r\n",
      "Collecting numpy\r\n",
      "  Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\r\n",
      "Requirement already satisfied: ml_dtypes in /opt/conda/lib/python3.10/site-packages (0.5.1)\r\n",
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.19.0)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (25.2.10)\r\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.4.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (16.0.0)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.20.3)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.31.0)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (59.8.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.3.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.6.3)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.14.1)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.51.1)\r\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.19.0)\r\n",
      "Requirement already satisfied: keras>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.10.0)\r\n",
      "  Using cached numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\r\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.14.0)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.37.1)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\r\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (13.4.2)\r\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\r\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.16.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2023.5.7)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.3)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.1)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (2.3.6)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.0.9)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\r\n",
      "Installing collected packages: numpy\r\n",
      "  Attempting uninstall: numpy\r\n",
      "    Found existing installation: numpy 1.25.0\r\n",
      "    Uninstalling numpy-1.25.0:\r\n",
      "      Successfully uninstalled numpy-1.25.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cudf 23.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "cuml 23.6.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "dask-cudf 23.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 2.1.3 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\r\n",
      "bqplot 0.12.39 requires numpy<2.0.0,>=1.10.4, but you have numpy 2.1.3 which is incompatible.\r\n",
      "cudf 23.6.1 requires protobuf<4.22,>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "cuml 23.6.0 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\r\n",
      "cupy 12.1.0 requires numpy<1.27,>=1.20, but you have numpy 2.1.3 which is incompatible.\r\n",
      "dask-cuda 23.6.0 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\r\n",
      "dask-cudf 23.6.1 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\r\n",
      "ibis-framework 6.0.0 requires numpy<2,>=1, but you have numpy 2.1.3 which is incompatible.\r\n",
      "momepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "numba 0.57.1 requires numpy<1.25,>=1.21, but you have numpy 2.1.3 which is incompatible.\r\n",
      "pymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 2.1.3 which is incompatible.\r\n",
      "pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.15.3 which is incompatible.\r\n",
      "raft-dask 23.6.2 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\r\n",
      "tensorflow-decision-forests 1.4.0 requires tensorflow~=2.12.0, but you have tensorflow 2.19.0 which is incompatible.\r\n",
      "tensorflow-text 2.12.1 requires tensorflow<2.13,>=2.12.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.19.0 which is incompatible.\r\n",
      "tensorflow-transform 0.14.0 requires numpy<2,>=1.16, but you have numpy 2.1.3 which is incompatible.\r\n",
      "ydata-profiling 4.3.1 requires numpy<1.24,>=1.16.0, but you have numpy 2.1.3 which is incompatible.\r\n",
      "ydata-profiling 4.3.1 requires scipy<1.11,>=1.4.1, but you have scipy 1.15.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed numpy-2.1.3\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (1.15.3)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\r\n",
      "Collecting scikit-learn\r\n",
      "  Downloading scikit_learn-1.7.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.5,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from scipy) (2.1.3)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\r\n",
      "Installing collected packages: scikit-learn\r\n",
      "  Attempting uninstall: scikit-learn\r\n",
      "    Found existing installation: scikit-learn 1.2.2\r\n",
      "    Uninstalling scikit-learn-1.2.2:\r\n",
      "      Successfully uninstalled scikit-learn-1.2.2\r\n",
      "Successfully installed scikit-learn-1.7.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade numpy ml_dtypes tensorflow\n",
    "!pip install --upgrade scipy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46908df6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T15:38:21.426003Z",
     "iopub.status.busy": "2025-07-20T15:38:21.425299Z",
     "iopub.status.idle": "2025-07-20T15:38:30.581617Z",
     "shell.execute_reply": "2025-07-20T15:38:30.580616Z"
    },
    "papermill": {
     "duration": 9.182931,
     "end_time": "2025-07-20T15:38:30.583783",
     "exception": false,
     "start_time": "2025-07-20T15:38:21.400852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf2onnx\r\n",
      "  Downloading tf2onnx-1.16.1-py3-none-any.whl (455 kB)\r\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m455.8/455.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.1 in /opt/conda/lib/python3.10/site-packages (from tf2onnx) (2.1.3)\r\n",
      "Requirement already satisfied: onnx>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from tf2onnx) (1.14.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from tf2onnx) (2.31.0)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from tf2onnx) (1.16.0)\r\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /opt/conda/lib/python3.10/site-packages (from tf2onnx) (25.2.10)\r\n",
      "Requirement already satisfied: protobuf~=3.20 in /opt/conda/lib/python3.10/site-packages (from tf2onnx) (3.20.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /opt/conda/lib/python3.10/site-packages (from onnx>=1.4.1->tf2onnx) (4.6.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->tf2onnx) (3.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->tf2onnx) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->tf2onnx) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->tf2onnx) (2023.5.7)\r\n",
      "Installing collected packages: tf2onnx\r\n",
      "Successfully installed tf2onnx-1.16.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tf2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2da258a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T15:38:30.634699Z",
     "iopub.status.busy": "2025-07-20T15:38:30.634076Z",
     "iopub.status.idle": "2025-07-20T15:38:32.976665Z",
     "shell.execute_reply": "2025-07-20T15:38:32.975227Z"
    },
    "papermill": {
     "duration": 2.370969,
     "end_time": "2025-07-20T15:38:32.978888",
     "exception": true,
     "start_time": "2025-07-20T15:38:30.607919",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 15:38:30.967677: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753025910.990710      25 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753025910.997797      25 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753025911.017463      25 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753025911.017487      25 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753025911.017490      25 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753025911.017492      25 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_25/3835834654.py\", line 2, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/__init__.py\", line 49, in <module>\n",
      "    from tensorflow._api.v2 import __internal__\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/_api/v2/__internal__/__init__.py\", line 8, in <module>\n",
      "    from tensorflow._api.v2.__internal__ import autograph\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py\", line 8, in <module>\n",
      "    from tensorflow.python.autograph.core.ag_ctx import control_status_ctx # line: 34\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/autograph/core/ag_ctx.py\", line 21, in <module>\n",
      "    from tensorflow.python.autograph.utils import ag_logging\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/autograph/utils/__init__.py\", line 17, in <module>\n",
      "    from tensorflow.python.autograph.utils.context_managers import control_dependency_on_returns\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/autograph/utils/context_managers.py\", line 19, in <module>\n",
      "    from tensorflow.python.framework import ops\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\", line 50, in <module>\n",
      "    from tensorflow.python.eager import context\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/context.py\", line 38, in <module>\n",
      "    from tensorflow.python.eager import execute\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py\", line 21, in <module>\n",
      "    from tensorflow.python.framework import dtypes\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/dtypes.py\", line 21, in <module>\n",
      "    import ml_dtypes\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ml_dtypes/__init__.py\", line 40, in <module>\n",
      "    from ml_dtypes._finfo import finfo\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ml_dtypes/_finfo.py\", line 153, in <module>\n",
      "    class finfo(np.finfo):  # pylint: disable=invalid-name,missing-class-docstring\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ml_dtypes/_finfo.py\", line 699, in finfo\n",
      "    _finfo_cache = {\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ml_dtypes/_finfo.py\", line 700, in <dictcomp>\n",
      "    t: init_fn.__func__() for t, init_fn in _finfo_type_map.items()  # pytype: disable=attribute-error\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ml_dtypes/_finfo.py\", line 668, in _float8_e8m0fnu_finfo\n",
      "    if not hasattr(obj, \"tiny\"):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/numpy/core/getlimits.py\", line 578, in tiny\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/numpy/core/getlimits.py\", line 557, in smallest_normal\n",
      "TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1428, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1319, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1172, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1087, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 969, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/executing/executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "# Tamil Handwriting Recognition - Pure Script Version (No Functions/Classes)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import tf2onnx\n",
    "import onnx\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d6e8b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T04:54:51.449313Z",
     "iopub.status.busy": "2025-07-20T04:54:51.448619Z",
     "iopub.status.idle": "2025-07-20T05:04:32.770190Z",
     "shell.execute_reply": "2025-07-20T05:04:32.769148Z",
     "shell.execute_reply.started": "2025-07-20T04:54:51.449281Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Load Training Data\n",
    "print(\"\\nüìä Preparing training dataset...\")\n",
    "DATA_PATH=\"/kaggle/input/tamil-hwcr\"\n",
    "train_csv_path = os.path.join(DATA_PATH, \"train.csv\")\n",
    "train_img_dir = os.path.join(DATA_PATH, \"Train-Kaggle\", \"Train-Kaggle\")\n",
    "df_train = pd.read_csv(train_csv_path)\n",
    "print(f\"Loaded {len(df_train)} records from CSV\")\n",
    "X_train_images = []\n",
    "y_train_raw = []\n",
    "skipped = 0\n",
    "for idx, row in df_train.iterrows():\n",
    "    img_id = row['ID']\n",
    "    label_value = row['Class Label']\n",
    "    img_path = os.path.join(train_img_dir, img_id)\n",
    "    if not os.path.exists(img_path):\n",
    "        skipped += 1\n",
    "        continue\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        skipped += 1\n",
    "        continue\n",
    "    IMG_WIDTH, IMG_HEIGHT=64,64\n",
    "    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    if img.ndim == 2:\n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "    elif img.ndim == 3 and img.shape[-1] != 1:\n",
    "        img = img[..., :1]  # force single channel\n",
    "    # Augmentation (optional, can be commented out)\n",
    "    if random.random() < 0.7:\n",
    "        # Brightness\n",
    "        if random.random() < 0.5:\n",
    "            brightness = random.uniform(-0.2, 0.2)\n",
    "            img = np.clip(img + brightness, 0, 1)\n",
    "        # Contrast\n",
    "        if random.random() < 0.5:\n",
    "            contrast = random.uniform(0.8, 1.2)\n",
    "            img = np.clip(img * contrast, 0, 1)\n",
    "        # Rotation\n",
    "        if random.random() < 0.5:\n",
    "            angle = random.uniform(-15, 15)\n",
    "            h, w = img.shape[:2]\n",
    "            center = (w // 2, h // 2)\n",
    "            rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "            img = cv2.warpAffine(img, rotation_matrix, (w, h), borderValue=1.0)\n",
    "        # Horizontal flip\n",
    "        if random.random() < 0.1:\n",
    "            img = cv2.flip(img, 1)\n",
    "        # Translation\n",
    "        if random.random() < 0.5:\n",
    "            shift_x = random.uniform(-0.1, 0.1) * img.shape[1]\n",
    "            shift_y = random.uniform(-0.1, 0.1) * img.shape[0]\n",
    "            translation_matrix = np.float32([[1, 0, shift_x], [0, 1, shift_y]])\n",
    "            img = cv2.warpAffine(img, translation_matrix, (img.shape[1], img.shape[0]), borderValue=1.0)\n",
    "        # Scaling\n",
    "        if random.random() < 0.5:\n",
    "            scale = random.uniform(0.9, 1.1)\n",
    "            h, w = img.shape[:2]\n",
    "            new_h, new_w = int(h * scale), int(w * scale)\n",
    "            img = cv2.resize(img, (new_w, new_h))\n",
    "            if new_h > h or new_w > w:\n",
    "                start_y = max(0, (new_h - h) // 2)\n",
    "                start_x = max(0, (new_w - w) // 2)\n",
    "                img = img[start_y:start_y+h, start_x:start_x+w]\n",
    "            else:\n",
    "                pad_y = (h - new_h) // 2\n",
    "                pad_x = (w - new_w) // 2\n",
    "                img = cv2.copyMakeBorder(img, pad_y, h-new_h-pad_y, pad_x, w-new_w-pad_x, cv2.BORDER_CONSTANT, value=1.0)\n",
    "        # Noise\n",
    "        if random.random() < 0.3:\n",
    "            noise = np.random.normal(0, 0.02, img.shape)\n",
    "            img = np.clip(img + noise, 0, 1)\n",
    "    if img.ndim == 2:\n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "    X_train_images.append(img)\n",
    "    y_train_raw.append(label_value)\n",
    "    if len(X_train_images) % 5000 == 0 and len(X_train_images) > 0:\n",
    "        print(f\"Processed {len(X_train_images)} images...\")\n",
    "print(f\"Successfully loaded {len(X_train_images)} images, skipped {skipped} due to errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee6341b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_images = np.array(X_train_images)\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train_raw)\n",
    "class_names = [str(cls) for cls in label_encoder.classes_]\n",
    "if len(label_encoder.classes_) != NUM_CLASSES:\n",
    "    print(f\"Warning: LabelEncoder found {len(label_encoder.classes_)} unique classes, but NUM_CLASSES is set to {NUM_CLASSES}.\")\n",
    "y_train = to_categorical(y_train_encoded, num_classes=NUM_CLASSES)\n",
    "print(f\"Training set: {X_train_images.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4548421f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Split for Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_images, y_train, test_size=0.2, random_state=42, stratify=np.argmax(y_train, axis=1)\n",
    ")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8638a52a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Build Model\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 1)),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"\\nModel Architecture:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480de460",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4. Train Model\n",
    "print(\"\\nüöÄ Starting training...\")\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True, verbose=1),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-7, verbose=1),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=str(OUTPUT_DIR / 'tamil_handwriting_best.h5'), monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "]\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45037d3d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 5. Evaluate on Validation Set\n",
    "print(\"\\nüìà Evaluating model on validation set...\")\n",
    "y_pred = model.predict(X_val, verbose=0)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_val, axis=1)\n",
    "accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
    "loss, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "target_names = TAMIL_CHARS[:NUM_CLASSES] if len(TAMIL_CHARS) >= NUM_CLASSES else TAMIL_CHARS + [f\"Class_{i}\" for i in range(len(TAMIL_CHARS), NUM_CLASSES)]\n",
    "report = classification_report(y_true_classes, y_pred_classes, target_names=target_names, zero_division=0)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names[:min(len(target_names), 20)], yticklabels=target_names[:min(len(target_names), 20)])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823b4b92",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 6. Plot Training History\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "ax1.set_title('Model Accuracy')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "ax2.plot(history.history['loss'], label='Training Loss')\n",
    "ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
    "ax2.set_title('Model Loss')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2eabdf",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 7. Load and Evaluate on Test Set (if labels present)\n",
    "print(\"\\nüìä Preparing test dataset...\")\n",
    "test_csv_path = os.path.join(DATA_PATH, \"test.csv\")\n",
    "test_img_dir = os.path.join(DATA_PATH, \"Test-Kaggle\", \"Test-Kaggle\")\n",
    "df_test = pd.read_csv(test_csv_path)\n",
    "print(f\"Loaded {len(df_test)} records from CSV\")\n",
    "X_test_images = []\n",
    "y_test_raw = []\n",
    "skipped = 0\n",
    "for idx, row in df_test.iterrows():\n",
    "    img_id = row['ID']\n",
    "    img_path = os.path.join(test_img_dir, img_id)\n",
    "    if not os.path.exists(img_path):\n",
    "        skipped += 1\n",
    "        continue\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        skipped += 1\n",
    "        continue\n",
    "    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    img = np.expand_dims(img, axis=-1)\n",
    "    X_test_images.append(img)\n",
    "    if 'Class Label' in row:\n",
    "        y_test_raw.append(row['Class Label'])\n",
    "    if len(X_test_images) % 1000 == 0 and len(X_test_images) > 0:\n",
    "        print(f\"Processed {len(X_test_images)} images...\")\n",
    "print(f\"Successfully loaded {len(X_test_images)} images, skipped {skipped} due to errors\")\n",
    "X_test_images = np.array(X_test_images)\n",
    "if y_test_raw:\n",
    "    y_test_encoded = label_encoder.transform(y_test_raw)\n",
    "    y_test = to_categorical(y_test_encoded, num_classes=NUM_CLASSES)\n",
    "    print(f\"Test set: {X_test_images.shape[0]} samples (labels present)\")\n",
    "    print(\"\\nüìà Evaluating model on test set...\")\n",
    "    y_pred_test = model.predict(X_test_images, verbose=0)\n",
    "    y_pred_classes_test = np.argmax(y_pred_test, axis=1)\n",
    "    y_true_classes_test = np.argmax(y_test, axis=1)\n",
    "    test_accuracy = accuracy_score(y_true_classes_test, y_pred_classes_test)\n",
    "    print(f\"Test set accuracy: {test_accuracy:.4f}\")\n",
    "else:\n",
    "    print(f\"Test set: {X_test_images.shape[0]} samples (no labels)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52a972e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 8. Save Model and Deployment Package\n",
    "print(\"\\nüì¶ Creating deployment package...\")\n",
    "model_name = \"tamil_handwriting_model\"\n",
    "keras_path = OUTPUT_DIR / f\"{model_name}.h5\"\n",
    "model.save(str(keras_path))\n",
    "print(f\"Model saved as: {keras_path}\")\n",
    "saved_model_path = OUTPUT_DIR / f\"{model_name}_savedmodel\"\n",
    "model.save(str(saved_model_path), save_format='tf')\n",
    "print(f\"SavedModel saved as: {saved_model_path}\")\n",
    "char_mapping = {\n",
    "    'char_to_idx': CHAR_TO_IDX,\n",
    "    'idx_to_char': IDX_TO_CHAR,\n",
    "    'tamil_chars': TAMIL_CHARS,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'img_height': IMG_HEIGHT,\n",
    "    'img_width': IMG_WIDTH\n",
    "}\n",
    "with open(OUTPUT_DIR / f\"{model_name}_char_mapping.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(char_mapping, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Character mapping saved as: {OUTPUT_DIR / f'{model_name}_char_mapping.json'}\")\n",
    "try:\n",
    "    onnx_path = OUTPUT_DIR / f\"{model_name}.onnx\"\n",
    "    input_signature = [tf.TensorSpec(shape=[None, IMG_HEIGHT, IMG_WIDTH, 1], dtype=tf.float32, name='input')]\n",
    "    onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature=input_signature, output_path=str(onnx_path))\n",
    "    print(f\"ONNX model saved as: {onnx_path}\")\n",
    "    onnx_model = onnx.load(str(onnx_path))\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print(\"ONNX model verification successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error converting to ONNX: {e}\")\n",
    "    print(\"Make sure tf2onnx is installed: pip install tf2onnx\")\n",
    "deployment_info = {\n",
    "    'model_info': {\n",
    "        'name': model_name,\n",
    "        'architecture': 'CNN',\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'input_shape': [IMG_HEIGHT, IMG_WIDTH, 1],\n",
    "        'accuracy': accuracy if 'accuracy' in locals() else 'Not available'\n",
    "    },\n",
    "    'preprocessing': {\n",
    "        'resize': [IMG_WIDTH, IMG_HEIGHT],\n",
    "        'normalize': 'divide by 255',\n",
    "        'grayscale': True\n",
    "    },\n",
    "    'postprocessing': {\n",
    "        'output_type': 'softmax_probabilities',\n",
    "        'classes': TAMIL_CHARS[:NUM_CLASSES]\n",
    "    },\n",
    "    'files': {\n",
    "        'keras_model': f\"{model_name}.h5\",\n",
    "        'savedmodel': f\"{model_name}_savedmodel\",\n",
    "        'onnx_model': f\"{model_name}.onnx\",\n",
    "        'char_mapping': f\"{model_name}_char_mapping.json\",\n",
    "        'deployment_info': f\"{model_name}_deployment_info.json\"\n",
    "    }\n",
    "}\n",
    "with open(OUTPUT_DIR / f\"{model_name}_deployment_info.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(deployment_info, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Deployment package created in: {OUTPUT_DIR}\")\n",
    "print(\"Files created:\")\n",
    "for file_type, filename in deployment_info['files'].items():\n",
    "    if filename:\n",
    "        print(f\"  - {file_type}: {filename}\")\n",
    "print(\"\\n‚úÖ Training completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc75b79b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 64\n",
    "NUM_CLASSES = 156\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "DATA_PATH = \"/kaggle/input/tamil-hwcr\"\n",
    "OUTPUT_DIR = Path(\"/kaggle/working\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "TAMIL_CHARS = [\n",
    "    '‡Æ∑‡Øç','‡Æ∑‡Æø', '‡Æ∑‡ØÄ', '‡Æ∑‡ØÅ', '‡Æ∑‡ØÇ',\n",
    "    '‡Æ∏‡Øç','‡Æ∏‡Æø', '‡Æ∏‡ØÄ', '‡Æ∏‡ØÅ', '‡Æ∏‡ØÇ',\n",
    "    '‡Æπ‡Øç','‡Æπ‡Æø', '‡Æπ‡ØÄ', '‡Æπ‡ØÅ', '‡Æπ‡ØÇ',\n",
    "    '‡Æú‡Øç','‡Æú‡Æø', '‡Æú‡ØÄ', '‡Æú‡ØÅ', '‡Æú‡ØÇ',\n",
    "    '‡Æï‡Øç‡Æ∑‡Øç', '‡Æï‡Øç‡Æ∑‡Æø', '‡Æï‡Øç‡Æ∑‡ØÄ', '‡Æï‡Øç‡Æ∑‡ØÅ', '‡Æï‡Øç‡Æ∑‡ØÇ',\n",
    "    '‡Æ∂‡Øç','‡Æ∂‡Æø', '‡Æ∂‡ØÄ', '‡Æ∂‡ØÅ', '‡Æ∂‡ØÇ',\n",
    "    '‡Ææ', '‡ØÜ', '‡Øá', '‡Øà',\n",
    "    '‡ÆÖ', '‡ÆÜ', '‡Æá', '‡Æà', '‡Æâ', '‡Æä', '‡Æé', '‡Æè', '‡Æê', '‡Æí', '‡Æì', '‡Æî',\n",
    "    '‡Æï', '‡Æô', '‡Æö', '‡Æû', '‡Æü', '‡Æ£', '‡Æ§', '‡Æ®', '‡Æ™', '‡ÆÆ',\n",
    "    '‡ÆØ', '‡Æ∞', '‡Æ≤', '‡Æµ', '‡Æ¥', '‡Æ≥', '‡Æ±', '‡Æ©','‡Æ∑',\n",
    "    '‡Æï‡Øç', '‡Æï‡Æø', '‡Æï‡ØÄ', '‡Æï‡ØÅ', '‡Æï‡ØÇ',\n",
    "    '‡Æô‡Øç', '‡Æô‡Æø', '‡Æô‡ØÄ', '‡Æô‡ØÅ', '‡Æô‡ØÇ',\n",
    "    '‡Æö‡Øç', '‡Æö‡Æø', '‡Æö‡ØÄ', '‡Æö‡ØÅ', '‡Æö‡ØÇ',\n",
    "    '‡Æû‡Øç', '‡Æû‡Æø', '‡Æû‡ØÄ', '‡Æû‡ØÅ', '‡Æû‡ØÇ',\n",
    "    '‡Æü‡Øç', '‡Æü‡Æø', '‡Æü‡ØÄ', '‡Æü‡ØÅ', '‡Æü‡ØÇ',\n",
    "    '‡Æ£‡Øç', '‡Æ£‡Æø', '‡Æ£‡ØÄ', '‡Æ£‡ØÅ', '‡Æ£‡ØÇ',\n",
    "    '‡Æ§‡Øç', '‡Æ§‡Æø', '‡Æ§‡ØÄ', '‡Æ§‡ØÅ', '‡Æ§‡ØÇ',\n",
    "    '‡Æ®‡Øç', '‡Æ®‡Æø', '‡Æ®‡ØÄ', '‡Æ®‡ØÅ', '‡Æ®‡ØÇ',\n",
    "    '‡Æ™‡Øç', '‡Æ™‡Æø', '‡Æ™‡ØÄ', '‡Æ™‡ØÅ', '‡Æ™‡ØÇ',\n",
    "    '‡ÆÆ‡Øç', '‡ÆÆ‡Æø', '‡ÆÆ‡ØÄ', '‡ÆÆ‡ØÅ', '‡ÆÆ‡ØÇ',\n",
    "    '‡ÆØ‡Øç', '‡ÆØ‡Æø', '‡ÆØ‡ØÄ', '‡ÆØ‡ØÅ', '‡ÆØ‡ØÇ',\n",
    "    '‡Æ∞‡Øç', '‡Æ∞‡Æø', '‡Æ∞‡ØÄ', '‡Æ∞‡ØÅ', '‡Æ∞‡ØÇ',\n",
    "    '‡Æ≤‡Øç', '‡Æ≤‡Æø', '‡Æ≤‡ØÄ', '‡Æ≤‡ØÅ', '‡Æ≤‡ØÇ',\n",
    "    '‡Æµ‡Øç', '‡Æµ‡Æø', '‡Æµ‡ØÄ', '‡Æµ‡ØÅ', '‡Æµ‡ØÇ',\n",
    "    '‡Æ¥‡Øç', '‡Æ¥‡Æø', '‡Æ¥‡ØÄ', '‡Æ¥‡ØÅ', '‡Æ¥‡ØÇ',\n",
    "    '‡Æ≥‡Øç', '‡Æ≥‡Æø', '‡Æ≥‡ØÄ', '‡Æ≥‡ØÅ', '‡Æ≥‡ØÇ',\n",
    "    '‡Æ±‡Øç', '‡Æ±‡Æø', '‡Æ±‡ØÄ', '‡Æ±‡ØÅ', '‡Æ±‡ØÇ',\n",
    "    '‡Æ©‡Øç', '‡Æ©‡Æø', '‡Æ©‡ØÄ', '‡Æ©‡ØÅ', '‡Æ©‡ØÇ'\n",
    "]\n",
    "CHAR_TO_IDX = {char: idx for idx, char in enumerate(TAMIL_CHARS)}\n",
    "IDX_TO_CHAR = {idx: char for idx, char in enumerate(TAMIL_CHARS)}\n",
    "\n",
    "print(\"üî§ Tamil Handwriting Recognition System\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Image dimensions: {IMG_HEIGHT}x{IMG_WIDTH}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081aede6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 6350438,
     "sourceId": 58786,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30528,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 167.569947,
   "end_time": "2025-07-20T15:38:33.925236",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-20T15:35:46.355289",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
